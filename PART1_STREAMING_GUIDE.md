# ğŸš€ PHáº¦N 1: REAL-TIME STREAMING PIPELINE

## ğŸ“‹ Tá»•ng quan

Pipeline real-time xá»­ lÃ½ dá»¯ liá»‡u tráº­n Ä‘áº¥u League of Legends theo luá»“ng:

```
Data Generator â†’ Kafka â†’ Spark Streaming â†’ Elasticsearch â†’ Kibana
```

### CÃ¡c thÃ nh pháº§n:
- **Data Generator**: Táº¡o dá»¯ liá»‡u tráº­n Ä‘áº¥u giáº£ láº­p
- **Kafka**: Message queue Ä‘á»ƒ stream data
- **Spark Streaming**: Xá»­ lÃ½ vÃ  transform data real-time
- **Elasticsearch**: LÆ°u trá»¯ vÃ  index data
- **Kibana**: Visualization vÃ  dashboard

---

## ğŸ¯ YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n cá»©ng
- **RAM**: Tá»‘i thiá»ƒu 8GB (khuyáº¿n nghá»‹ 16GB)
- **CPU**: 4 cores trá»Ÿ lÃªn
- **Disk**: 50GB trá»‘ng

### Pháº§n má»m
- **Docker Desktop**: Version 20.10+
- **Python**: 3.8+
- **PowerShell**: 5.1+ (Windows) hoáº·c Bash (Linux/Mac)

### Ports cáº§n thiáº¿t
- `9092`: Kafka
- `9200`: Elasticsearch
- `5601`: Kibana
- `8080`: Spark Master UI
- `4040`: Spark Application UI
- `9870`: HDFS NameNode (optional)

---

## ğŸ“¦ BÆ°á»›c 1: CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

### 1.1. Clone repository

```bash
git clone <repository-url>
cd LOL-BIGDATA
```

### 1.2. Táº¡o Python virtual environment

```powershell
# Windows
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

### 1.3. CÃ i Ä‘áº·t dependencies

```powershell
pip install -r requirements.txt
```

**LÆ°u Ã½**: File `requirements.txt` Ä‘Ã£ bao gá»“m táº¥t cáº£ packages cáº§n thiáº¿t:
- kafka-python
- elasticsearch
- pyspark
- pandas, numpy
- faker (Ä‘á»ƒ generate data)

---

## ğŸ³ BÆ°á»›c 2: Build Custom Spark Image

**Quan trá»ng**: BÆ°á»›c nÃ y chá»‰ cáº§n cháº¡y **1 láº§n duy nháº¥t** khi setup láº§n Ä‘áº§u.

### 2.1. Build image

```powershell
.\build_spark_image.ps1
```

**Thá»i gian**: ~5-10 phÃºt (tÃ¹y tá»‘c Ä‘á»™ máº¡ng)

Script nÃ y sáº½:
- Build custom Spark image vá»›i Python packages pre-installed
- Install Kafka JARs
- Tá»‘i Æ°u Ä‘á»ƒ Spark job start nhanh (<5 giÃ¢y)

### 2.2. Verify image

```powershell
docker images | Select-String "spark-custom"
```

Káº¿t quáº£ mong Ä‘á»£i:
```
spark-custom   3.5.0   <image-id>   <size>
```

---

## ğŸš€ BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng Infrastructure

### 3.1. Start Docker containers

```powershell
docker-compose up -d
```

**Äá»£i 60 giÃ¢y** Ä‘á»ƒ táº¥t cáº£ services khá»Ÿi Ä‘á»™ng hoÃ n toÃ n.

### 3.2. Verify containers

```powershell
docker ps
```

Pháº£i tháº¥y cÃ¡c containers:
- âœ… `kafka`
- âœ… `zookeeper`
- âœ… `elasticsearch`
- âœ… `kibana`
- âœ… `spark-master`
- âœ… `spark-worker`

### 3.3. Check container health

```powershell
# Check Kafka
docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# Check Elasticsearch
curl http://localhost:9200

# Check Kibana
curl http://localhost:5601/api/status
```

---

## ğŸ“Š BÆ°á»›c 4: Táº¡o Kafka Topic

### 4.1. Create topic

```powershell
docker exec kafka kafka-topics --create `
  --bootstrap-server localhost:9092 `
  --topic lol_matches `
  --partitions 3 `
  --replication-factor 1
```

### 4.2. Verify topic

```powershell
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

Káº¿t quáº£ mong Ä‘á»£i:
```
lol_matches
```

### 4.3. Describe topic (optional)

```powershell
docker exec kafka kafka-topics --describe --bootstrap-server localhost:9092 --topic lol_matches
```

---

## ğŸ” BÆ°á»›c 5: Táº¡o Elasticsearch Index

### 5.1. Create index vá»›i mapping

```powershell
.\create_es_index.ps1
```

Script nÃ y sáº½:
- Táº¡o index `lol_matches_stream`
- Set mapping cho cÃ¡c fields
- Configure analyzers

### 5.2. Verify index

```powershell
curl http://localhost:9200/lol_matches_stream
```

---

## ğŸ² BÆ°á»›c 6: Start Data Generator

### 6.1. Activate virtual environment (náº¿u chÆ°a)

```powershell
.\.venv\Scripts\Activate.ps1
```

### 6.2. Start generator

```powershell
python lol_match_generator.py
```

**Output máº«u**:
```
2026-01-15 14:30:01 - INFO - Starting LoL Match Data Generator
2026-01-15 14:30:01 - INFO - Connected to Kafka: localhost:29092
2026-01-15 14:30:02 - INFO - âœ“ Sent match 1/100 to Kafka
2026-01-15 14:30:03 - INFO - âœ“ Sent match 2/100 to Kafka
...
```

### 6.3. Verify data trong Kafka

Má»Ÿ terminal má»›i vÃ  cháº¡y:

```powershell
docker exec kafka kafka-console-consumer `
  --bootstrap-server localhost:9092 `
  --topic lol_matches `
  --max-messages 1
```

Báº¡n sáº½ tháº¥y JSON data cá»§a 1 tráº­n Ä‘áº¥u.

**LÆ°u Ã½**: Äá»ƒ generator cháº¡y trong background, giá»¯ terminal nÃ y má»Ÿ.

---

## âš¡ BÆ°á»›c 7: Start Spark Streaming Job

### 7.1. Má»Ÿ terminal má»›i

Giá»¯ terminal cá»§a Data Generator cháº¡y, má»Ÿ terminal má»›i.

### 7.2. Submit Spark job

```powershell
.\submit_spark_job.ps1
```

**Script nÃ y sáº½**:
- Submit Spark job vá»›i config tá»‘i Æ°u
- KhÃ´ng cáº§n install packages (Ä‘Ã£ cÃ³ trong custom image)
- Start streaming tá»« Kafka â†’ Elasticsearch
- Instant startup (<5 giÃ¢y)

### 7.3. Monitor Spark job

**Output máº«u**:
```
===========================================================
  Submitting Spark Streaming Job (OPTIMIZED - Instant)
===========================================================

Job Configuration:
   Master: spark://spark-master:7077
   Driver Memory: 2g
   Executor Memory: 2g
   Executor Cores: 2
   Mode: Structured Streaming

Submitting job...
2026-01-15 14:35:01 INFO SparkContext: Running Spark version 3.5.0
2026-01-15 14:35:02 INFO StreamExecution: Starting new streaming query
...
```

### 7.4. Verify Spark UI

Má»Ÿ browser:
- **Spark Master UI**: http://localhost:8080
- **Spark Application UI**: http://localhost:4040

Táº¡i Application UI, báº¡n sáº½ tháº¥y:
- Streaming tab vá»›i batch processing
- Input rate, processing time
- Number of records processed

---

## ğŸ“ˆ BÆ°á»›c 8: Verify Data trong Elasticsearch

### 8.1. Check document count

```powershell
curl http://localhost:9200/lol_matches_stream/_count
```

Káº¿t quáº£:
```json
{
  "count": 150,  // Sá»‘ lÆ°á»£ng tÄƒng dáº§n
  "_shards": {...}
}
```

### 8.2. Query sample data

```powershell
curl http://localhost:9200/lol_matches_stream/_search?size=1
```

### 8.3. Monitor indexing rate

```powershell
# Cháº¡y nhiá»u láº§n Ä‘á»ƒ tháº¥y count tÄƒng
while ($true) {
    $count = (curl http://localhost:9200/lol_matches_stream/_count | ConvertFrom-Json).count
    Write-Host "Documents: $count" -ForegroundColor Cyan
    Start-Sleep -Seconds 5
}
```

---

## ğŸ“Š BÆ°á»›c 9: Visualize vá»›i Kibana

### 9.1. Má»Ÿ Kibana

Browser: http://localhost:5601

### 9.2. Create Index Pattern

1. Menu â†’ **Stack Management** â†’ **Index Patterns**
2. Click **Create index pattern**
3. Index pattern name: `lol_matches_stream*`
4. Click **Next step**
5. Time field: `@timestamp`
6. Click **Create index pattern**

### 9.3. Explore Data

1. Menu â†’ **Discover**
2. Select index pattern: `lol_matches_stream*`
3. Báº¡n sáº½ tháº¥y data real-time

### 9.4. Create Visualizations

#### Visualization 1: Match Count Over Time

1. Menu â†’ **Visualize** â†’ **Create visualization**
2. Type: **Line**
3. Index: `lol_matches_stream*`
4. Metrics:
   - Y-axis: Count
5. Buckets:
   - X-axis: Date Histogram
   - Field: @timestamp
   - Interval: Auto
6. **Save**: "Match Count Over Time"

#### Visualization 2: Top Champions

1. Create visualization â†’ **Pie**
2. Metrics: Count
3. Buckets:
   - Split slices: Terms
   - Field: champion_name.keyword
   - Size: 10
4. **Save**: "Top 10 Champions"

#### Visualization 3: Win Rate by Team

1. Create visualization â†’ **Metric**
2. Add filters:
   - Filter 1: `win: true`
   - Filter 2: `team_id: 100`
3. **Save**: "Blue Team Win Rate"

### 9.5. Create Dashboard

1. Menu â†’ **Dashboard** â†’ **Create dashboard**
2. Add visualizations:
   - Match Count Over Time
   - Top 10 Champions
   - Blue Team Win Rate
3. **Save**: "LoL Real-time Dashboard"

### 9.6. Enable Auto-refresh

1. Trong Dashboard, click biá»ƒu tÆ°á»£ng Ä‘á»“ng há»“ (top-right)
2. Set auto-refresh: **5 seconds**
3. Báº¡n sáº½ tháº¥y dashboard update real-time!

---

## ğŸ” Monitoring & Troubleshooting

### Monitor Logs

```powershell
# Kafka logs
docker logs kafka -f --tail 100

# Spark logs
docker logs spark-master -f --tail 100

# Elasticsearch logs
docker logs elasticsearch -f --tail 100
```

### Common Issues

#### Issue 1: Kafka khÃ´ng nháº­n data

**Triá»‡u chá»©ng**: Generator cháº¡y nhÆ°ng khÃ´ng tháº¥y data trong Kafka

**Giáº£i phÃ¡p**:
```powershell
# Check Kafka connectivity
docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092

# Restart Kafka
docker-compose restart kafka
```

#### Issue 2: Spark job bá»‹ lá»—i "partitions are gone"

**Triá»‡u chá»©ng**: 
```
ERROR: Set(lol_matches-2, lol_matches-1) are gone
```

**Giáº£i phÃ¡p**: Script `start_spark_streaming.ps1` Ä‘Ã£ tá»± Ä‘á»™ng fix. Náº¿u váº«n lá»—i:
```powershell
# Manual cleanup
docker exec spark-master rm -rf /opt/spark/work-dir/checkpoints/streaming
.\start_spark_streaming.ps1
```

#### Issue 3: Elasticsearch khÃ´ng nháº­n data

**Triá»‡u chá»©ng**: Spark cháº¡y OK nhÆ°ng ES count = 0

**Giáº£i phÃ¡p**:
```powershell
# Check ES health
curl http://localhost:9200/_cluster/health

# Check index
curl http://localhost:9200/_cat/indices?v

# Recreate index
.\create_es_index.ps1
```

#### Issue 4: Kibana khÃ´ng hiá»ƒn thá»‹ data

**Triá»‡u chá»©ng**: Index pattern created nhÆ°ng khÃ´ng tháº¥y data

**Giáº£i phÃ¡p**:
1. Check time range (top-right): Set to "Last 15 minutes"
2. Refresh index pattern: Stack Management â†’ Index Patterns â†’ Refresh
3. Check @timestamp field exists

---

## ğŸ›‘ Dá»«ng há»‡ thá»‘ng

### Dá»«ng tá»«ng thÃ nh pháº§n

```powershell
# 1. Stop Data Generator
# Ctrl+C trong terminal cá»§a generator

# 2. Stop Spark Job
# Ctrl+C trong terminal cá»§a Spark

# 3. Stop Docker containers
docker-compose down
```

### Dá»«ng vÃ  xÃ³a data (Clean start)

```powershell
# Stop vÃ  xÃ³a volumes
docker-compose down -v

# Hoáº·c chá»‰ xÃ³a Kafka data
docker-compose down
docker volume rm lol-bigdata_kafka_data
```

---

## ğŸ”„ Restart há»‡ thá»‘ng

### Restart giá»¯ data cÅ©

```powershell
# 1. Start containers
docker-compose up -d
Start-Sleep -Seconds 60

# 2. Start generator (terminal 1)
.\.venv\Scripts\Activate.ps1
python lol_match_generator.py

# 3. Start Spark (terminal 2)
.\start_spark_streaming.ps1
```

### Restart clean (xÃ³a data cÅ©)

```powershell
# 1. Stop vÃ  xÃ³a Kafka data
docker-compose down
docker volume rm lol-bigdata_kafka_data

# 2. Start containers
docker-compose up -d
Start-Sleep -Seconds 60

# 3. Recreate topic
docker exec kafka kafka-topics --create --bootstrap-server localhost:9092 --topic lol_matches --partitions 3 --replication-factor 1

# 4. Recreate ES index
.\create_es_index.ps1

# 5. Start generator
python lol_match_generator.py

# 6. Start Spark
.\start_spark_streaming.ps1
```

---

## ğŸ“Š Performance Metrics

### Expected Performance

| Metric | Value |
|--------|-------|
| Data generation rate | ~1 match/second |
| Kafka throughput | ~100 messages/second |
| Spark processing time | <1 second/batch |
| End-to-end latency | <5 seconds |
| Elasticsearch indexing | ~50 docs/second |

### Monitor Performance

```powershell
# Kafka lag
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group spark-streaming

# Spark metrics
# Check http://localhost:4040/streaming/

# Elasticsearch indexing rate
curl http://localhost:9200/_nodes/stats/indices/indexing
```

---

## ğŸ¯ Success Criteria

Há»‡ thá»‘ng cháº¡y thÃ nh cÃ´ng khi:

- âœ… Data Generator táº¡o data liÃªn tá»¥c
- âœ… Kafka nháº­n vÃ  lÆ°u messages
- âœ… Spark job process data khÃ´ng lá»—i
- âœ… Elasticsearch document count tÄƒng dáº§n
- âœ… Kibana dashboard hiá»ƒn thá»‹ data real-time
- âœ… Auto-refresh hoáº¡t Ä‘á»™ng mÆ°á»£t mÃ 

---

## ğŸ“ Files quan trá»ng

| File | Má»¥c Ä‘Ã­ch |
|------|----------|
| `start_spark_streaming.ps1` | **Main script** - Start Spark vá»›i auto-clean checkpoint |
| `lol_match_generator.py` | Generate fake match data |
| `docker-compose.yml` | Infrastructure definition |
| `streaming-layer/src/spark_streaming_consumer.py` | Spark streaming logic |
| `streaming-layer/config/spark_config_docker.yaml` | Spark configuration |
| `create_es_index.ps1` | Create Elasticsearch index |

---

## ğŸš€ Next Steps

Sau khi hoÃ n thÃ nh Pháº§n 1, báº¡n cÃ³ thá»ƒ:

1. **TÃ¹y chá»‰nh data generation**:
   - Sá»­a `lol_match_generator.py`
   - Thay Ä‘á»•i rate, sá»‘ lÆ°á»£ng matches

2. **Tá»‘i Æ°u Spark processing**:
   - Sá»­a `spark_config_docker.yaml`
   - Adjust batch interval, memory

3. **Táº¡o thÃªm visualizations**:
   - Heatmaps
   - Aggregations
   - Custom metrics

4. **Chuyá»ƒn sang Pháº§n 2**: Batch Processing Layer

---

## ğŸ’¡ Tips & Best Practices

1. **LuÃ´n dÃ¹ng `start_spark_streaming.ps1`** thay vÃ¬ submit trá»±c tiáº¿p
   - Tá»± Ä‘á»™ng clean checkpoint
   - TrÃ¡nh lá»—i offset

2. **Monitor logs thÆ°á»ng xuyÃªn**
   - PhÃ¡t hiá»‡n lá»—i sá»›m
   - Optimize performance

3. **Backup Kibana dashboards**
   - Export saved objects
   - Version control

4. **Clean restart khi cÃ³ váº¥n Ä‘á»**
   - XÃ³a volumes
   - Recreate tá»« Ä‘áº§u

5. **Check Docker resources**
   - Docker Desktop â†’ Settings â†’ Resources
   - Ensure 8GB+ RAM allocated

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸ‰**

Náº¿u gáº·p váº¥n Ä‘á», check pháº§n Troubleshooting hoáº·c xem logs Ä‘á»ƒ debug.
