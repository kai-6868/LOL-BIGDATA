# CODEBASE STRUCTURE - Big Data LoL System

## ğŸ“ Tá»•ng quan cáº¥u trÃºc thÆ° má»¥c

```
lol-bigdata-system/
â”‚
â”œâ”€â”€ ğŸ“ data-generator/              # Data generation & Kafka producer
â”œâ”€â”€ ğŸ“ streaming-layer/             # Real-time processing
â”œâ”€â”€ ğŸ“ batch-layer/                 # Batch processing
â”œâ”€â”€ ğŸ“ ml-layer/                    # Machine Learning pipeline
â”œâ”€â”€ ğŸ“ infrastructure/              # Docker, configs, IaC
â”œâ”€â”€ ğŸ“ monitoring/                  # Monitoring & alerting
â”œâ”€â”€ ğŸ“ tests/                       # End-to-end tests
â”œâ”€â”€ ğŸ“ docs/                        # Documentation
â”œâ”€â”€ ğŸ“ notebooks/                   # Jupyter notebooks
â”œâ”€â”€ ğŸ“ scripts/                     # Utility scripts
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ Makefile
```

---

## ğŸ“‚ CHI TIáº¾T Cáº¤U TRÃšC

### 1. ğŸ“ data-generator/ (Currently in root)

**Current Status:** âœ… **IMPLEMENTED** (Single file in root)

```
lol_match_generator.py  # Standalone script (root level)
```

**Current Features:**

- âœ… Riot API v2 format compatibility
- âœ… 36 champion pool (hardcoded)
- âœ… Realistic match statistics generation
- âœ… Kafka producer integration (kafka-python)
- âœ… Continuous generation mode
- âœ… Configurable interval (INTERVAL_SECONDS = 0.5)
- âœ… Fixed topic: 'lol_matches'

**Implementation Details:**

```python
# Key configurations:
BOOTSTRAP_SERVERS = 'localhost:9092'
TOPIC_NAME = 'lol_matches'
INTERVAL_SECONDS = 0.5  # 2 matches/second

# Data structure:
- CHAMPIONS: 36 champions list
- POSITIONS: [TOP, JUNGLE, MIDDLE, BOTTOM, UTILITY]
- Match format: Riot API compatible
  - metadata: matchId, participants PUUIDs
  - info: gameCreation, gameDuration, participants, teams
```

**Planned Structure (Future Refactoring):**

```
data-generator/
â”œâ”€â”€ lol_match_generator.py          # Main script âœ… (move from root)
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ champions.json              # Extract champion list
â”‚   â”œâ”€â”€ kafka_config.yaml           # Externalize Kafka config
â”‚   â””â”€â”€ generator_config.yaml       # Externalize settings
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ riot_api_v2_schema.json     # Document format
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_generator.py           # Unit tests
â””â”€â”€ README.md                       # Usage documentation
```

**CÃ´ng nghá»‡:**

- Python 3.9+ âœ…
- kafka-python âœ…
- json, random, time (stdlib) âœ…
- Future: PyYAML, Faker (for enhancement)

---

### 2. ğŸ“ streaming-layer/

```
streaming-layer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ spark_streaming_app.py      # Main Spark Streaming app
â”‚   â”œâ”€â”€ consumers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ kafka_consumer.py       # Kafka integration
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ window_aggregator.py    # Windowing logic
â”‚   â”‚   â”œâ”€â”€ stats_calculator.py     # Statistics calculation
â”‚   â”‚   â””â”€â”€ enrichment.py           # Data enrichment
â”‚   â”œâ”€â”€ sinks/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ elasticsearch_sink.py   # ES writer
â”‚   â”‚   â””â”€â”€ console_sink.py         # Debug output
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ spark_session.py        # Spark session factory
â”‚       â””â”€â”€ monitoring.py           # Metrics collection
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ spark_config.yaml           # Spark configuration
â”‚   â”œâ”€â”€ elasticsearch_config.yaml   # ES connection
â”‚   â””â”€â”€ window_config.yaml          # Window settings
â”‚
â”œâ”€â”€ kibana/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ realtime_overview.json
â”‚   â”‚   â”œâ”€â”€ champion_stats.json
â”‚   â”‚   â””â”€â”€ win_rate_analysis.json
â”‚   â””â”€â”€ visualizations/
â”‚       â”œâ”€â”€ time_series.json
â”‚       â””â”€â”€ pie_charts.json
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_streaming_app.py
â”‚   â”œâ”€â”€ test_processors.py
â”‚   â””â”€â”€ test_sinks.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- Apache Spark 3.4+ (Structured Streaming)
- PySpark
- Elasticsearch-py
- Kafka-python

---

### 3. ğŸ“ batch-layer/

```
batch-layer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch_consumer.py           # Kafka to HDFS
â”‚   â”œâ”€â”€ pyspark_etl.py              # Main ETL job
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extract_job.py          # Data extraction
â”‚   â”‚   â”œâ”€â”€ transform_job.py        # Transformation logic
â”‚   â”‚   â””â”€â”€ load_job.py             # Load to Cassandra
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ flatten_json.py         # JSON flattening
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py    # Feature extraction
â”‚   â”‚   â””â”€â”€ aggregator.py           # Data aggregation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ hdfs_helper.py          # HDFS operations
â”‚       â”œâ”€â”€ cassandra_helper.py     # Cassandra operations
â”‚       â””â”€â”€ date_utils.py           # Date handling
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ hdfs_config.yaml            # HDFS settings
â”‚   â”œâ”€â”€ cassandra_config.yaml       # Cassandra connection
â”‚   â”œâ”€â”€ spark_batch_config.yaml     # Spark batch config
â”‚   â””â”€â”€ schedule.yaml               # Job scheduling
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ cassandra_schema.cql        # Cassandra DDL
â”‚   â”œâ”€â”€ create_keyspace.cql
â”‚   â””â”€â”€ analytics_queries.cql       # Sample queries
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_batch_consumer.py
â”‚   â”œâ”€â”€ test_etl_job.py
â”‚   â””â”€â”€ test_transformers.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- Apache Spark (Batch mode)
- PySpark
- Hadoop HDFS
- Cassandra Driver
- Airflow (scheduling)

---

### 4. ğŸ“ ml-layer/

```
ml-layer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ feature_engineering.py      # Feature engineering
â”‚   â”œâ”€â”€ model_training.py           # Model training
â”‚   â”œâ”€â”€ model_prediction.py         # Batch prediction
â”‚   â”œâ”€â”€ model_evaluation.py         # Model evaluation
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ player_features.py      # Player-level features
â”‚   â”‚   â”œâ”€â”€ champion_features.py    # Champion features
â”‚   â”‚   â””â”€â”€ team_features.py        # Team features
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ random_forest_model.py  # RF implementation
â”‚   â”‚   â”œâ”€â”€ model_registry.py       # Model versioning
â”‚   â”‚   â””â”€â”€ serving.py              # Model serving API
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py              # Custom metrics
â”‚       â””â”€â”€ visualization.py        # Plot functions
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ features_config.yaml        # Feature definitions
â”‚   â”œâ”€â”€ model_config.yaml           # Model hyperparameters
â”‚   â””â”€â”€ mlflow_config.yaml          # MLflow settings
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb                # Exploratory analysis
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_model_evaluation.ipynb
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest_v1.pkl
â”‚   â”œâ”€â”€ feature_scaler.pkl
â”‚   â””â”€â”€ label_encoder.pkl
â”‚
â”œâ”€â”€ mlflow/
â”‚   â”œâ”€â”€ experiment_tracking.py
â”‚   â””â”€â”€ model_registry.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_features.py
â”‚   â”œâ”€â”€ test_model_training.py
â”‚   â””â”€â”€ test_prediction.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- Scikit-learn
- MLflow
- XGBoost (alternative model)
- Pandas, NumPy
- Matplotlib, Seaborn

---

### 5. ğŸ“ infrastructure/

```
infrastructure/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ server.properties
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ spark-defaults.conf
â”‚   â”œâ”€â”€ elasticsearch/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ elasticsearch.yml
â”‚   â”œâ”€â”€ cassandra/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ cassandra.yaml
â”‚   â””â”€â”€ jupyter/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ jupyter_notebook_config.py
â”‚
â”œâ”€â”€ kubernetes/                      # (Optional) K8s manifests
â”‚   â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ elasticsearch/
â”‚   â””â”€â”€ cassandra/
â”‚
â”œâ”€â”€ terraform/                       # (Optional) IaC
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â”‚
â”œâ”€â”€ docker-compose.yml               # Main compose file
â”œâ”€â”€ docker-compose.dev.yml           # Dev override
â”œâ”€â”€ docker-compose.prod.yml          # Prod override
â”‚
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- Docker & Docker Compose
- Kubernetes (optional)
- Terraform (optional)
- Apache Zookeeper

---

### 6. ğŸ“ monitoring/

```
monitoring/
â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ prometheus.yml              # Prometheus config
â”‚   â”œâ”€â”€ alerts.yml                  # Alert rules
â”‚   â””â”€â”€ targets.json                # Scrape targets
â”‚
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ kafka_metrics.json
â”‚   â”‚   â”œâ”€â”€ spark_metrics.json
â”‚   â”‚   â”œâ”€â”€ elasticsearch_metrics.json
â”‚   â”‚   â”œâ”€â”€ cassandra_metrics.json
â”‚   â”‚   â””â”€â”€ ml_model_metrics.json
â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â””â”€â”€ prometheus.yaml
â”‚   â””â”€â”€ grafana.ini
â”‚
â”œâ”€â”€ exporters/
â”‚   â”œâ”€â”€ kafka_exporter.py
â”‚   â”œâ”€â”€ spark_exporter.py
â”‚   â””â”€â”€ custom_metrics.py
â”‚
â”œâ”€â”€ alertmanager/
â”‚   â””â”€â”€ alertmanager.yml
â”‚
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- Prometheus
- Grafana
- Alertmanager
- Node Exporter
- JMX Exporter

---

### 7. ğŸ“ tests/

```
tests/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_end_to_end.py          # Full pipeline test
â”‚   â”œâ”€â”€ test_streaming_to_es.py
â”‚   â””â”€â”€ test_batch_to_cassandra.py
â”‚
â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ kafka_load_test.py          # Kafka throughput
â”‚   â”œâ”€â”€ es_query_load_test.py       # ES performance
â”‚   â””â”€â”€ cassandra_write_test.py     # Cassandra write perf
â”‚
â”œâ”€â”€ data_quality/
â”‚   â”œâ”€â”€ test_schema_validation.py
â”‚   â”œâ”€â”€ test_data_completeness.py
â”‚   â””â”€â”€ test_data_accuracy.py
â”‚
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ sample_matches.json
â”‚   â””â”€â”€ test_data.json
â”‚
â””â”€â”€ README.md
```

**CÃ´ng nghá»‡:**

- pytest
- pytest-benchmark
- Locust (load testing)
- Great Expectations (data quality)

---

### 8. ğŸ“ docs/

```
docs/
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ system_design.md
â”‚   â”œâ”€â”€ data_flow.md
â”‚   â”œâ”€â”€ component_diagram.png
â”‚   â””â”€â”€ sequence_diagram.png
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â”œâ”€â”€ kafka_topics.md
â”‚   â”œâ”€â”€ elasticsearch_indices.md
â”‚   â””â”€â”€ cassandra_schema.md
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ local_setup.md
â”‚   â”œâ”€â”€ docker_deployment.md
â”‚   â”œâ”€â”€ cloud_deployment.md
â”‚   â””â”€â”€ kubernetes_deployment.md
â”‚
â”œâ”€â”€ guides/
â”‚   â”œâ”€â”€ getting_started.md
â”‚   â”œâ”€â”€ development_guide.md
â”‚   â”œâ”€â”€ troubleshooting.md
â”‚   â””â”€â”€ best_practices.md
â”‚
â””â”€â”€ README.md
```

---

### 9. ğŸ“ notebooks/

```
notebooks/
â”œâ”€â”€ exploration/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_kafka_consumer_test.ipynb
â”‚   â””â”€â”€ 03_elasticsearch_queries.ipynb
â”‚
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ champion_analysis.ipynb
â”‚   â”œâ”€â”€ win_rate_trends.ipynb
â”‚   â””â”€â”€ player_performance.ipynb
â”‚
â””â”€â”€ README.md
```

---

### 10. ğŸ“ scripts/

```
scripts/
â”œâ”€â”€ setup/
â”‚   â”œâ”€â”€ init_kafka.sh               # Kafka topic creation
â”‚   â”œâ”€â”€ init_elasticsearch.sh       # ES index setup
â”‚   â”œâ”€â”€ init_cassandra.sh           # Cassandra keyspace
â”‚   â””â”€â”€ setup_all.sh                # Full setup
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ deploy_streaming.sh
â”‚   â”œâ”€â”€ deploy_batch.sh
â”‚   â””â”€â”€ deploy_ml.sh
â”‚
â”œâ”€â”€ maintenance/
â”‚   â”œâ”€â”€ backup_cassandra.sh
â”‚   â”œâ”€â”€ clean_old_data.sh
â”‚   â””â”€â”€ health_check.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ load_sample_data.py
â”‚   â””â”€â”€ generate_test_data.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“¦ ROOT FILES

### requirements.txt

```txt
# Core dependencies
kafka-python==2.0.2
pyspark==3.4.1
elasticsearch==8.9.0
cassandra-driver==3.28.0
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
mlflow==2.5.0

# Data generation
Faker==19.2.0
avro-python3==1.10.2

# Monitoring
prometheus-client==0.17.1
grafana-client==3.5.0

# Testing
pytest==7.4.0
pytest-benchmark==4.0.0
locust==2.15.1
great-expectations==0.17.12

# Utilities
PyYAML==6.0
python-dotenv==1.0.0
click==8.1.6
rich==13.5.2

# Development
jupyter==1.0.0
black==23.7.0
flake8==6.1.0
mypy==1.4.1
```

### docker-compose.yml

```yaml
version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./infrastructure/docker/hadoop/hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./infrastructure/docker/hadoop/hadoop.env

  spark-master:
    image: bitnami/spark:3.4.1
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master

  spark-worker:
    image: bitnami/spark:3.4.1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false

  kibana:
    image: docker.elastic.co/kibana/kibana:8.9.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

  cassandra:
    image: cassandra:4.1
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=lol_cluster

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards

  jupyter:
    build: ./infrastructure/docker/jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
```

### Makefile

```makefile
.PHONY: help setup start stop clean test

help:
	@echo "Available commands:"
	@echo "  make setup    - Setup infrastructure"
	@echo "  make start    - Start all services"
	@echo "  make stop     - Stop all services"
	@echo "  make test     - Run tests"
	@echo "  make clean    - Clean up everything"

setup:
	docker-compose up -d
	./scripts/setup/setup_all.sh

start:
	docker-compose up -d
	python data-generator/src/lol_match_generator.py &
	spark-submit streaming-layer/src/spark_streaming_app.py &

stop:
	docker-compose down

test:
	pytest tests/ -v

clean:
	docker-compose down -v
	rm -rf data/*
	rm -rf logs/*
```

---

## ğŸ”§ CÃ”NG NGHá»† STACK SUMMARY

| Layer                  | CÃ´ng nghá»‡              | Version | Má»¥c Ä‘Ã­ch               |
| ---------------------- | ---------------------- | ------- | ---------------------- |
| **Data Ingestion**     | Apache Kafka           | 3.5     | Message broker         |
|                        | Zookeeper              | 3.8     | Kafka coordination     |
| **Stream Processing**  | Apache Spark Streaming | 3.4     | Real-time processing   |
| **Batch Processing**   | Apache Spark (Batch)   | 3.4     | Batch ETL              |
|                        | Hadoop HDFS            | 3.3     | Data lake storage      |
| **Search & Analytics** | Elasticsearch          | 8.9     | Search engine          |
|                        | Kibana                 | 8.9     | Visualization          |
| **Database**           | Apache Cassandra       | 4.1     | NoSQL storage          |
| **Machine Learning**   | Scikit-learn           | 1.3     | ML algorithms          |
|                        | MLflow                 | 2.5     | ML lifecycle           |
| **Monitoring**         | Prometheus             | Latest  | Metrics collection     |
|                        | Grafana                | Latest  | Dashboards             |
| **Container**          | Docker                 | Latest  | Containerization       |
|                        | Docker Compose         | Latest  | Orchestration          |
| **Language**           | Python                 | 3.9+    | Primary language       |
| **Testing**            | pytest                 | 7.4     | Unit/integration tests |
|                        | Locust                 | 2.15    | Load testing           |

---

## ğŸš€ QUICK START

```bash
# 1. Clone repository
git clone <repo-url>
cd lol-bigdata-system

# 2. Setup infrastructure
make setup

# 3. Start all services
make start

# 4. Access dashboards
# Kibana: http://localhost:5601
# Grafana: http://localhost:3000
# Spark UI: http://localhost:8080
# Jupyter: http://localhost:8888

# 5. Run tests
make test
```

---

**Next Steps**: Báº¯t Ä‘áº§u implement tá»« Phase 1 trong PLANMODE.md
