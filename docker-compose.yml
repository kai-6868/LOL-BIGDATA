version: "3.8"

services:
  # =========================
  # ZOOKEEPER
  # =========================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - lol-network

  # =========================
  # KAFKA BROKER 1
  # =========================
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-1
    hostname: kafka-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:19092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_IN_SYNC_REPLICAS: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data_1:/var/lib/kafka/data
    networks:
      - lol-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5

  # =========================
  # KAFKA BROKER 2
  # =========================
  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-2
    hostname: kafka-2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
      - "29093:29093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:19093,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-2:19093,EXTERNAL://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_IN_SYNC_REPLICAS: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data_2:/var/lib/kafka/data
    networks:
      - lol-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9093"]
      interval: 30s
      timeout: 10s
      retries: 5

  # =========================
  # KAFKA BROKER 3
  # =========================
  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-3
    hostname: kafka-3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
      - "29094:29094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:19094,EXTERNAL://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-3:19094,EXTERNAL://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_IN_SYNC_REPLICAS: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data_3:/var/lib/kafka/data
    networks:
      - lol-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9094"]
      interval: 30s
      timeout: 10s
      retries: 5

  # =========================
  # HADOOP NAMENODE
  # =========================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      CLUSTER_NAME: lol-cluster
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - lol-network

  # =========================
  # HADOOP DATANODE 1
  # =========================
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - lol-network

  # =========================
  # HADOOP DATANODE 2
  # =========================
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    depends_on:
      - namenode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    networks:
      - lol-network

  # =========================
  # SPARK MASTER
  # =========================
  spark-master:
    image: spark-custom:3.5.0
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_NO_DAEMONIZE=1
    volumes:
      - ./streaming-layer:/opt/spark/work-dir/streaming-layer
      - ./data-generator:/opt/spark/work-dir/data-generator
      - ./checkpoints:/opt/spark/work-dir/checkpoints
    entrypoint: ["/opt/spark/bin/spark-class"]
    command:
      [
        "org.apache.spark.deploy.master.Master",
        "--host",
        "spark-master",
        "--port",
        "7077",
        "--webui-port",
        "8080",
      ]
    networks:
      - lol-network

  # =========================
  # SPARK WORKER
  # =========================
  spark-worker:
    image: spark-custom:3.5.0
    container_name: spark-worker
    hostname: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_NO_DAEMONIZE=1
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_CORES=2
    volumes:
      - ./streaming-layer:/opt/spark/work-dir/streaming-layer
      - ./data-generator:/opt/spark/work-dir/data-generator
      - ./checkpoints:/opt/spark/work-dir/checkpoints
    entrypoint: ["/opt/spark/bin/spark-class"]
    command:
      [
        "org.apache.spark.deploy.worker.Worker",
        "spark://spark-master:7077",
        "--webui-port",
        "8081",
        "--memory",
        "4g",
        "--cores",
        "2",
      ]
    networks:
      - lol-network

  # =========================
  # ELASTICSEARCH
  # =========================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: "-Xms2g -Xmx2g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - lol-network

  # =========================
  # KIBANA
  # =========================
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    networks:
      - lol-network

  # =========================
  # CASSANDRA
  # =========================
  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    ports:
      - "9042:9042"
    environment:
      CASSANDRA_CLUSTER_NAME: lol-cluster
      CASSANDRA_DC: dc1
      CASSANDRA_RACK: rack1
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - lol-network

  # =========================
  # PROMETHEUS
  # =========================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - lol-network

  # =========================
  # GRAFANA
  # =========================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - lol-network

  # =========================
  # PYSPARK WORKER
  # =========================
  pyspark-worker:
    image: python:3.11-slim
    container_name: pyspark-worker
    volumes:
      - ./batch-layer:/opt/batch-layer
    networks:
      - lol-network
    depends_on:
      - namenode
      - cassandra
    working_dir: /opt/batch-layer
    command: >
      bash -c "
      apt-get update -qq &&
      apt-get install -y -qq default-jre-headless procps &&
      pip install --quiet --no-cache-dir pyspark==3.5.0 cassandra-driver pyyaml pandas pyarrow &&
      echo 'âœ… PySpark worker ready!' &&
      tail -f /dev/null
      "

# =========================
# NETWORKS & VOLUMES
# =========================
networks:
  lol-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_datanode2:
  kafka_data_1:
  kafka_data_2:
  kafka_data_3:
  es_data:
  cassandra_data:
  prometheus_data:
  grafana_data:
